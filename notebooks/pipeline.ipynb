{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "49ac752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yoonapark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yoonapark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af64d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_path = '../data/yelp_academic_dataset_review.json'\n",
    "\n",
    "data = []\n",
    "with open(yelp_path, 'r') as f:\n",
    "  for line in f.readlines():\n",
    "    data.append(json.loads(line))\n",
    "    \n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1b151d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge u...</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly Cardenas Salon!  I'm always a fan of a great blowo...</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I have to say that this office really has it together, they are so organized and friendly!  Dr. ...</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delicious, and the Caesar salad had an absolutely delici...</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Today was my second out of three sessions I had paid for. Although my first session went well, I...</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q  hG7b0MtEbXx5QzbzE6C_VA  ujmEBvifdJM6h6RLv4wQIg   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q  yXQM5uF2jS6es16SJzNHfg  NZnhc2sEQy3RmzKTZnqtwQ   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig  dacAIZ6fTM6mqwW5uxkskg  ikCg8xy5JIg_NGPx-MSIDA   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw  ssoyf2_x0EQMed6fgHeMyQ  b1b1eb3uo-w561D0ZfCEiQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    1.0       6      1     0   \n",
       "1    5.0       0      0     0   \n",
       "2    5.0       3      0     0   \n",
       "3    5.0       0      0     0   \n",
       "4    1.0       7      0     0   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge u...   \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly Cardenas Salon!  I'm always a fan of a great blowo...   \n",
       "2  I have to say that this office really has it together, they are so organized and friendly!  Dr. ...   \n",
       "3  Went in for a lunch. Steak sandwich was delicious, and the Caesar salad had an absolutely delici...   \n",
       "4  Today was my second out of three sessions I had paid for. Although my first session went well, I...   \n",
       "\n",
       "                  date  \n",
       "0  2013-05-07 04:34:36  \n",
       "1  2017-01-14 21:30:33  \n",
       "2  2016-11-09 20:09:03  \n",
       "3  2018-01-09 20:56:38  \n",
       "4  2018-01-30 23:07:38  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "19a663df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiBERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        super(SentiBERT, self).__init__()\n",
    "        \n",
    "        hidden_dim = kargs['hidden_dim']\n",
    "        self.bert = AutoModel.from_pretrained(kargs['model_name_or_path'])\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        self.layer1 = nn.Sequential(nn.Linear(hidden_dim,hidden_dim),\n",
    "                                   nn.ReLU())\n",
    "        self.layer2 = nn.Linear(hidden_dim, 3)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        \"\"\"\n",
    "        x: x is a list of token tensors, already tokenized by the tokenizer\n",
    "        \"\"\"\n",
    "        x = self.bert(x)\n",
    "        last_hidden_state_cls = x.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        x = self.layer1(torch.squeeze(last_hidden_state_cls))\n",
    "        out = self.layer2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "948f3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stars_to_sentiment(x):\n",
    "    return 2 if x >= 4.0 else (0 if x <2.0 else 1)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return ' '.join([word for word in word_tokenize(text) if not word.lower() in stop_words])\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, output_path):\n",
    "        self.text = df['text']\n",
    "        self.labels = df['stars'].apply(lambda x: stars_to_sentiment(x))\n",
    "        \n",
    "        # Remove stop words\n",
    "        filtered_text = df['text'].apply(lambda x: tokenize_text(x))\n",
    "        \n",
    "        # tokenization\n",
    "        self.text = tokenizer.batch_encode_plus(filtered_text.tolist(), truncation=True, \n",
    "                                                     add_special_tokens=True, padding='max_length', max_length=max_length)['input_ids']\n",
    "        \n",
    "        \n",
    "        data_path = Path(output_path, 'tokenized_data.pkl')\n",
    "        if not Path(data_path).exists():\n",
    "            with open(data_path, 'wb') as f:\n",
    "                pickle.dump({'text':self.text, 'labels':self.labels}, f)\n",
    "\n",
    "        print(f'Loaded {len(self.labels)} examples.')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.text[index],dtype=torch.long), torch.tensor(self.labels[index], dtype=torch.long)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "033a8490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 examples.\n"
     ]
    }
   ],
   "source": [
    "model_name='bert-base-uncased'\n",
    "max_length = 512\n",
    "batch_size = 10\n",
    "output_path = '../data/'\n",
    "num_epoch = 4\n",
    "learning_rate = 3e-4\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SentiBERT(model_name_or_path=model_name, hidden_dim=768).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = ReviewDataset(data_df, tokenizer, 512, output_path)\n",
    "train_set_size = int(len(dataset) * 0.8)\n",
    "test_set_size = len(dataset) - train_set_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_set_size, test_set_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "32018eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total loss at epoch 1 : 80.72475\n",
      "The average loss at 1 : 80.72475\n",
      "The total loss at epoch 2 : 72.78884\n",
      "The average loss at 2 : 72.78884\n",
      "The total loss at epoch 3 : 71.81705\n",
      "The average loss at 3 : 71.81705\n",
      "The total loss at epoch 4 : 72.06914\n",
      "The average loss at 4 : 72.06914\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    loss = 0\n",
    "    total_batch = 0\n",
    "    for batch, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch.to(device))\n",
    "        batch_loss = criterion(output, labels)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += batch_loss.item() * batch_size\n",
    "        total_batch += batch_size\n",
    "    \n",
    "    print(f'The total loss at epoch {epoch +1} : {loss:.5f}')\n",
    "    print(f'The average loss at {epoch +1} : {loss:.5f}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "857b970c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9f/279qm8hj0gd3pn88_lyzrty00000gn/T/ipykernel_72608/2479195920.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcorrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtestset_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    testset_size = 0\n",
    "    corrects = 0\n",
    "    for batch, labels in test_dataloader:\n",
    "        logits = model(batch.to(device))\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = probs.argmax(dim=1)\n",
    "        corrects += (pred == labels)\n",
    "        testset_size += batch_size\n",
    "    accuracy = corrects.sum().float() / float(testset_size)\n",
    "    print(f'The accuracy of the testset is {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b84ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dapt",
   "language": "python",
   "name": "dapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
